{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MDo09j2MaFyt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "xpVOpDOqaX6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57286e1a-05d2-4101-9a05-4a7610fa5d9d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "train_data = pd.read_csv(r'/content/gdrive/My Drive/Colab Notebooks/rusentitweet_train.csv')\n",
        "test_data = pd.read_csv(r'/content/gdrive/My Drive/Colab Notebooks/rusentitweet_test.csv')\n",
        "\n",
        "true_labels = ['positive', 'negative']\n",
        "filtered_train_data = train_data[train_data['label'].isin(true_labels)]\n",
        "filtered_test_data = test_data[test_data['label'].isin(true_labels)]\n",
        "\n",
        "filtered_train_data.head(6)"
      ],
      "metadata": {
        "id": "1Qn47NigagkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Очистка текста\n",
        "def cleaner(tweet):\n",
        "  #Удаление ссылок на аккаунты\n",
        "  tweet = re.sub(r'@[\\w]+', '', tweet)\n",
        "\n",
        "  #Удаление обычных ссылок\n",
        "  tweet = re.sub(r'https?://\\S+', '', tweet)\n",
        "\n",
        "  #Удаление спецсимволов \"^\", \"&\", \"*\", \"|\", \"/\"\n",
        "  tweet = re.sub(r'[\\^&*/|]', '', tweet)\n",
        "\n",
        "  #Удаление хэштегов\n",
        "  tweet = re.sub(r'#\\w+', '', tweet)\n",
        "\n",
        "  return tweet\n",
        "\n",
        "cleaned_train_data = filtered_train_data.copy()\n",
        "cleaned_test_data = filtered_test_data.copy()\n",
        "\n",
        "cleaned_train_data['text'] = cleaned_train_data['text'].apply(cleaner)\n",
        "cleaned_test_data['text'] = cleaned_test_data['text'].apply(cleaner)\n",
        "\n",
        "cleaned_train_data.head(6)"
      ],
      "metadata": {
        "id": "rXHsw2pJxVJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#Осуществим стемминг подготовленного набора данных\n",
        "stemmer = SnowballStemmer('russian')\n",
        "def stemming(tweet):\n",
        "  words_arr = tweet.split()\n",
        "  words_arr = list(map(lambda w: stemmer.stem(w), words_arr))\n",
        "  return ' '.join(words_arr)\n",
        "\n",
        "# Применение стемминга\n",
        "stemmed_train_data = cleaned_train_data.copy()\n",
        "stemmed_test_data = cleaned_test_data.copy()\n",
        "stemmed_train_data['text'] = stemmed_train_data['text'].apply(stemming)\n",
        "stemmed_test_data['text'] = stemmed_test_data['text'].apply(stemming)\n",
        "\n",
        "stemmed_train_data.head(6)"
      ],
      "metadata": {
        "id": "RS1-RYHAaQL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Преобразование в мешок слов\n",
        "vectorizer = CountVectorizer()\n",
        "stemmed_train_data_counts = vectorizer.fit_transform(stemmed_train_data['text'])\n",
        "stemmed_test_data_counts = vectorizer.transform(stemmed_test_data['text'])\n",
        "print(stemmed_train_data_counts)"
      ],
      "metadata": {
        "id": "49PMVRCfl7L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#TF-IDF\n",
        "tfidf = TfidfTransformer()\n",
        "#Обучение на тренировочных данных\n",
        "d_train_tfidf = tfidf.fit_transform(stemmed_train_data_counts)\n",
        "#Преобразование текста в Tf-Idf матрицу на тестовых данных\n",
        "d_test_tfidf = tfidf.transform(stemmed_test_data_counts)\n",
        "\n",
        "print(d_test_tfidf)"
      ],
      "metadata": {
        "id": "e3e3aNXpqCSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Обучаем модели логистической регрессии и случайного леса на обучающей выборке, применяем их к тестовым данным\n",
        "label_maps = {\"positive\":1, \"negative\":0}\n",
        "\n",
        "#Сопоставление меток с числами\n",
        "D_train_encoded = stemmed_train_data['label'].map(label_maps)\n",
        "D_test_encoded = stemmed_test_data['label'].map(label_maps)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#Логистическая регрессия\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(d_train_tfidf, D_train_encoded)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Случайный лес\n",
        "rand_forest = RandomForestClassifier()\n",
        "rand_forest.fit(d_train_tfidf, D_train_encoded)\n",
        "\n",
        "#Предсказания\n",
        "predict_lin_reg_train = log_reg.predict(d_train_tfidf)\n",
        "predict_rand_for_train = rand_forest.predict(d_train_tfidf)\n",
        "\n",
        "predict_lin_reg_test = log_reg.predict(d_test_tfidf)\n",
        "predict_rand_for_test = rand_forest.predict(d_test_tfidf)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "#выводим полученные результаты по показателям\n",
        "print(\"Обучающие данные\\n\")\n",
        "print(\"Линейная регрессия\\n\", classification_report(D_train_encoded,predict_lin_reg_train))\n",
        "print(\"Случайный лес\\n\", classification_report(D_train_encoded,predict_rand_for_train))\n",
        "\n",
        "print(\"Тестовые данные\\n\")\n",
        "print(\"Линейная регрессия\\n\", classification_report(D_test_encoded,predict_lin_reg_test))\n",
        "print(\"Случайный лес\\n\", classification_report(D_test_encoded,predict_rand_for_test))"
      ],
      "metadata": {
        "id": "Z04tkWT4vshZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Выявляем наиболее важные признаки(слова)\n",
        "n = 15\n",
        "\n",
        "log_reg_coef = log_reg.coef_[0]\n",
        "log_reg_ind = abs(log_reg_coef).argsort()[::-1]\n",
        "log_reg_names = vectorizer.get_feature_names_out()\n",
        "log_reg_top_names = [log_reg_names[index] for index in log_reg_ind[:n]]\n",
        "print(\"Топ 25 наиболее значимых слов для логистической регрессии:\\n\", log_reg_top_names)\n",
        "\n",
        "rand_forest_coef = rand_forest.feature_importances_\n",
        "rand_forest_ind = abs(rand_forest_coef).argsort()[::-1]\n",
        "rand_forest_names = vectorizer.get_feature_names_out()\n",
        "rand_forest_top_names = [rand_forest_names[index] for index in rand_forest_ind[:n]]\n",
        "print(\"Топ 25 наиболее значимых слов для случайного леса:\\n\", rand_forest_top_names)\n"
      ],
      "metadata": {
        "id": "PjIK8d4VGKAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6384a1-8f73-497a-96bb-ab4d2dbc0394"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Топ 25 наиболее значимых слов для логистической регрессии:\n",
            " ['не', 'любл', 'хорош', 'пиздец', 'блят', 'красив', 'нет', 'нрав', 'мил', 'классн', 'нах', 'рад', 'крут', 'хорошо', 'прекрасн']\n",
            "Топ 25 наиболее значимых слов для случайного леса:\n",
            " ['не', 'так', 'любл', 'хорош', 'эт', 'что', 'как', 'пиздец', 'нет', 'красив', 'ты', 'блят', 'мо', 'все', 'на']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymystem3 import Mystem\n",
        "#Лемматизация\n",
        "m = Mystem()\n",
        "def lemmatization(tweet):\n",
        "  lem = m.lemmatize(tweet)\n",
        "  lemmas = []\n",
        "  for l in lem:\n",
        "    if l.isalnum():\n",
        "      lemmas.append(l)\n",
        "  lem_tweet = ' '.join(lemmas)\n",
        "  return lem_tweet\n",
        "\n",
        "# Применение лемматизации\n",
        "lem_train_data = cleaned_train_data.copy()\n",
        "lem_test_data = cleaned_test_data.copy()\n",
        "lem_train_data['text'] = lem_train_data['text'].apply(lemmatization)\n",
        "lem_test_data['text'] = lem_test_data['text'].apply(lemmatization)\n",
        "\n",
        "lem_train_data.head(50)"
      ],
      "metadata": {
        "id": "8qxjicfSOt5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Всё то же самое, но с лемматизацией\n",
        "\n",
        "#Преобразование в мешок слов\n",
        "vectorizer = CountVectorizer()\n",
        "lem_train_data_counts = vectorizer.fit_transform(lem_train_data['text'])\n",
        "lem_test_data_counts = vectorizer.transform(lem_test_data['text'])\n",
        "print(lem_train_data_counts)\n",
        "\n",
        "#################\n",
        "\n",
        "#TF-IDF\n",
        "tfidf = TfidfTransformer()\n",
        "#Обучение на тренировочных данных\n",
        "d_train_tfidf = tfidf.fit_transform(lem_train_data_counts)\n",
        "#Преобразование текста в Tf-Idf матрицу на тестовых данных\n",
        "d_test_tfidf = tfidf.transform(lem_test_data_counts)\n",
        "\n",
        "print(d_test_tfidf)\n",
        "\n",
        "#################\n",
        "\n",
        "#Обучаем модели логистической регрессии и случайного леса на обучающей выборке, применяем их к тестовым данным\n",
        "label_maps = {\"positive\":1, \"negative\":0}\n",
        "\n",
        "#Сопоставление меток с числами\n",
        "D_train_encoded = lem_train_data['label'].map(label_maps)\n",
        "D_test_encoded = lem_test_data['label'].map(label_maps)\n",
        "\n",
        "#Логистическая регрессия\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(d_train_tfidf, D_train_encoded)\n",
        "\n",
        "#Случайный лес\n",
        "rand_forest = RandomForestClassifier()\n",
        "rand_forest.fit(d_train_tfidf, D_train_encoded)\n",
        "\n",
        "#Предсказания\n",
        "predict_lin_reg_train = log_reg.predict(d_train_tfidf)\n",
        "predict_rand_for_train = rand_forest.predict(d_train_tfidf)\n",
        "\n",
        "predict_lin_reg_test = log_reg.predict(d_test_tfidf)\n",
        "predict_rand_for_test = rand_forest.predict(d_test_tfidf)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "#выводим полученные результаты по показателям\n",
        "print(\"Обучающие данные\\n\")\n",
        "print(\"Линейная регрессия\\n\", classification_report(D_train_encoded,predict_lin_reg_train))\n",
        "print(\"Случайный лес\\n\", classification_report(D_train_encoded,predict_rand_for_train))\n",
        "\n",
        "print(\"Тестовые данные\\n\")\n",
        "print(\"Линейная регрессия\\n\", classification_report(D_test_encoded,predict_lin_reg_test))\n",
        "print(\"Случайный лес\\n\", classification_report(D_test_encoded,predict_rand_for_test))\n",
        "\n",
        "#######################\n",
        "\n",
        "#Выявляем наиболее важные признаки(слова)\n",
        "n = 15\n",
        "\n",
        "log_reg_coef = log_reg.coef_[0]\n",
        "log_reg_ind = abs(log_reg_coef).argsort()[::-1]\n",
        "log_reg_names = vectorizer.get_feature_names_out()\n",
        "log_reg_top_names = [log_reg_names[index] for index in log_reg_ind[:n]]\n",
        "print(\"Топ 25 наиболее значимых слов для логистической регрессии:\\n\", log_reg_top_names)\n",
        "\n",
        "rand_forest_coef = rand_forest.feature_importances_\n",
        "rand_forest_ind = abs(rand_forest_coef).argsort()[::-1]\n",
        "rand_forest_names = vectorizer.get_feature_names_out()\n",
        "rand_forest_top_names = [rand_forest_names[index] for index in rand_forest_ind[:n]]\n",
        "print(\"Топ 25 наиболее значимых слов для случайного леса:\\n\", rand_forest_top_names)"
      ],
      "metadata": {
        "id": "4fHkOrN7Q8Qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}